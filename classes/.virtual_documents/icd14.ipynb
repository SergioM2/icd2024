





import pandas as pd
import numpy as np
from collections import defaultdict, Counter





class NaiveBayesClassifier:
    def __init__(self):
        self.class_priors = {}  # Prior probabilities of the classes
        self.likelihoods = {}   # Conditional probabilities (likelihoods)
        self.classes = None     # Unique classes in the dataset
        self.features = None    # Features (attributes)
    
    def fit(self, X, y):

        # Get the unique classes and features (attributes)
        self.classes = np.unique(y)
        self.features = X.columns
        total_samples = len(y)  # Total number of training instances
        
        # Estimate prior probabilities (relative frequency of each class)
        class_counts = y.value_counts().to_dict()
        self.class_priors = {cls: (class_counts[cls] / total_samples) for cls in self.classes}
        
        # Initialize conditional probabilities (likelihoods)
        self.likelihoods = {cls: {} for cls in self.classes}
        
        # Calculate the likelihoods (conditional probabilities) for each feature
        for cls in self.classes:
            X_cls = X[y == cls]  # Filter instances where the class is 'cls'
            total_cls_samples = len(X_cls)  # Number of instances per class
            
            # Calculate the likelihoods for each attribute and attribute value
            for feature in self.features:
                feature_counts = X_cls[feature].value_counts().to_dict()  # Frequency of each attribute value
                total_feature_values = len(X[feature].unique())  # Total number of possible attribute values
                
                # Apply Laplace smoothing and calculate the likelihoods
                self.likelihoods[cls][feature] = {
                    value: (feature_counts.get(value, 0) + 1) / (total_cls_samples + total_feature_values)
                    for value in X[feature].unique()
                }
    
    def predict(self, X_test):
        
        results = []
        
        # Iterate over each test instance
        for _, x in X_test.iterrows():
            class_probabilities = {}  # Store the posterior probabilities for each class
            
            # Calculate the posterior probability for each class
            for cls in self.classes:
                # Initialize with the prior probability of the class
                prob = self.class_priors[cls]
                
                # Multiply by the likelihoods (conditional probabilities) of each feature
                for feature in self.features:
                    value = x[feature]
                    prob *= self.likelihoods[cls][feature].get(value, 1 / (len(self.likelihoods[cls][feature]) + len(self.features)))
                
                # Store the calculated probability for the class
                class_probabilities[cls] = prob
            
            # Select the class with the highest posterior probability
            predicted_class = max(class_probabilities, key=class_probabilities.get)
            results.append(predicted_class)
        
        return results





data = pd.read_csv('weather.nominal.csv')

# Define X (features) and y (label)
X = data.iloc[:, :-1]  # All columns except the last one
y = data.iloc[:, -1]  # Last column (label)

# Train the classifier using the Naive Bayes algorithm with the original column names
nb_classifier = NaiveBayesClassifier()
nb_classifier.fit(X, y)

# Create the instance to test: sunny, hot, normal, TRUE
test_instance = pd.DataFrame([{
    'outlook': 'sunny',
    'temperature': 'cool',
    'humidity': 'high',
    'windy': True
}])

# Make the prediction
prediction = nb_classifier.predict(test_instance)
print(f"Prediction for the instance {test_instance.iloc[0].to_dict()}: {prediction[0]}")








from sklearn.naive_bayes import GaussianNB





df = pd.read_csv(r'weather.numeric.csv')





print(df)


# defining the dependent and independent variables
X_train = df[['Outlook', 'Temperature', 'Humidity', 'Wind']]
y_train = df[['Play']]

print(X_train.head())
print(y_train.head())





from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

outlook = X_train.iloc[:,0]
outlook_enc = encoder.fit_transform(outlook)
print(outlook.tolist())
print(outlook_enc)

wind = X_train.iloc[:,3]
wind_enc = encoder.fit_transform(wind)
print(wind.tolist())
print(wind_enc)


df_outlook = pd.DataFrame(outlook_enc, columns = ['Outlook'])
df_wind = pd.DataFrame(outlook_enc, columns = ['Wind'])
X_train_num = pd.concat([df_outlook, X_train.iloc[:,1], X_train.iloc[:,2], df_wind], axis=1)
print(X_train_num)








clf = GaussianNB().fit(X_train_num, y_train)





# sunny:2, hot:85, normal:65, strong:0 
new_example = [[2, 60, 65, 1]]
X_test = pd.DataFrame(new_example, columns = ['Outlook', 'Temperature', 'Humidity', 'Wind'])
print(X_test)
clf.predict(X_test)
