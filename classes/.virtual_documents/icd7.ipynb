





#pip install spacy
#pip install nltk
#pip install sklearn





import pandas as pd
import re
import spacy as spc
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer





oracion = "@Mar18blink Estoy buscando un producto genial por menos de 50. He encontrado muchos en https://t.co/FI1ShlDozc. ¿Qué opinas? Quiero comprarlo ya!!!"
print(oracion)








oracion = oracion.lower()
oracion = re.sub(r"@\S+", "", oracion)  # Eliminar menciones a usuarios
oracion = re.sub("http[s]?\://\S+", "", oracion)  # Eliminar enlaces
oracion = re.sub(r"#\S+", "", oracion)  # Eliminar hashtags
oracion = re.sub(r"[0-9]", "", oracion)  # Eliminar números
oracion = re.sub(r"(\(.*\))|(\[.*\])", "", oracion)  # Eliminar paréntesis y corchetes
oracion = re.sub(r"\n", "", oracion)  # Eliminar caracteres de nueva línea
oracion = re.sub(r"(http[s]?\://\S+)|([\[\(].*[\)\]])|([#@]\S+)|\n", "", oracion)  # Eliminar varios patrones
oracion = re.sub(r"(\.)|(,)", "", oracion)  # Eliminar puntos y comas
oracion = re.sub(r"[¡!]", "", oracion)  # Eliminar signos de admiración 
oracion = re.sub(r"[¿?]", "", oracion)  # Eliminar signos de exclamación
print(oracion)





#nltk.download('punkt')
tokens = word_tokenize(oracion)
print(tokens)





#nltk.download('stopwords')
spanish_stopwords = stopwords.words('spanish')
palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]
print(palabras_filtradas)





#sudo python3 -m spacy download es


nlp = spc.load("es_core_news_sm")
lema = nlp(" ".join(palabras_filtradas))
oracion_lematizada = " ".join([token.lemma_ for token in lema])
print(oracion_lematizada)





vectorizador = CountVectorizer()
vectores = vectorizador.fit_transform([oracion_lematizada])
vocabulario = vectorizador.get_feature_names_out()





print("Oración de entrada:", oracion)
print("Oración lematizada:", oracion_lematizada)
print("Vectores Bag of Words:", vectores.toarray())
print("Vocabulario:", vocabulario)





df_bw = pd.DataFrame.sparse.from_spmatrix(vectores,columns = vocabulario)
df_bw.head()



