





import numpy as np
import pandas as pd
import math
from collections import Counter








def entropy(y):
    # Cuenta la frecuencia de una variable
    counter = Counter(y)
    total = len(y)
    
    # Calcula la entropía
    ent = 0.0
    for count in counter.values():
        probability = count / total
        ent -= probability * math.log2(probability)
    return ent





def information_gain(X_column, y):
    # Entropía del conjunto original
    ent_total = entropy(y)
    
    # Dividir los datos por el valor de la característica
    values, counts = np.unique(X_column, return_counts=True)
    
    # Entropía ponderada de los subconjuntos
    weighted_entropy = 0.0
    for value, count in zip(values, counts):
        subset_y = y[X_column == value]
        if len(subset_y) == 0:  # Verificamos si el subconjunto está vacío
            continue
        weighted_entropy += (count / len(y)) * entropy(subset_y)
    
    # Ganancia de información
    return ent_total - weighted_entropy





def best_attribute(X, y):
    best_gain = -1
    best_attr = None
    
    for col in X.columns:
        gain = information_gain(X[col], y)
        if gain > best_gain:
            best_gain = gain
            best_attr = col
    return best_attr





class Node:
    def __init__(self, feature=None, value=None, children=None, *, label=None):
        self.feature = feature  # El nombre del atributo
        self.value = value  # El valor del atributo
        self.children = children if children is not None else {}  # Subárboles
        self.label = label  # Clase terminal

def id3(X, y):
    if len(np.unique(y)) == 1:  # Si todas las etiquetas son iguales
        return Node(label=np.unique(y)[0])
    
    if X.empty:  # Si no hay más atributos
        return Node(label=Counter(y).most_common(1)[0][0])  # Retornar la clase mayoritaria
    
    # Seleccionamos el mejor atributo
    best_attr = best_attribute(X, y)
    
    # Creamos el nodo
    root = Node(feature=best_attr)
    
    # Para cada valor posible del mejor atributo, creamos un subárbol
    values = np.unique(X[best_attr])
    for value in values:
        subset_X = X[X[best_attr] == value].drop(columns=[best_attr])
        subset_y = y[X[best_attr] == value]
        
        if len(subset_X) == 0:
            continue
        
        # Llamada recursiva a id3
        subtree = id3(subset_X, subset_y)
        root.children[value] = subtree
    
    return root





def predict(tree, X_test):
    if tree.label is not None:  # Nodo hoja
        return tree.label
    
    value = X_test[tree.feature]  
    if value in tree.children:
        return predict(tree.children[value], X_test)
    else:
        return None





data = pd.read_csv('weather.nominal.csv')

# Definir X (características) e y (etiqueta)
X = data.iloc[:, :-1]  # Todas las columnas menos la última
y = data.iloc[:, -1]  # Última columna (etiqueta)

# Entrenar el árbol de decisión usando el algoritmo ID3 con los nombres originales de las columnas
tree = id3(X, y)

# Crear la instancia para probar: sunny, hot, normal, TRUE
test_instance = {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'normal', 'windy': True}

# Realizar la predicción
prediction = predict(tree, test_instance)
print(f'Predicción para la instancia {test_instance}: {prediction}')








from sklearn.tree import DecisionTreeClassifier





df = pd.read_csv(r'weather.numeric.csv')





print(df)


# defining the dependent and independent variables
X_train = df[['Outlook', 'Temperature', 'Humidity', 'Wind']]
y_train = df[['Play']]

print(X_train.head())
print(y_train.head())








from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

outlook = X_train.iloc[:,0]
outlook_enc = encoder.fit_transform(outlook)
print(outlook.tolist())
print(outlook_enc)

wind = X_train.iloc[:,3]
wind_enc = encoder.fit_transform(wind)
print(wind.tolist())
print(wind_enc)


df_outlook = pd.DataFrame(outlook_enc, columns = ['Outlook'])
df_wind = pd.DataFrame(outlook_enc, columns = ['Wind'])
X_train_num = pd.concat([df_outlook, X_train.iloc[:,1], X_train.iloc[:,2], df_wind], axis=1)
print(X_train_num)





clf = DecisionTreeClassifier().fit(X_train_num, y_train)





from sklearn import tree
features = X_train_num.columns.values.tolist()
text_representation = tree.export_text(clf, feature_names = features)
print(text_representation)


from matplotlib import pyplot as plt
fig = tree.plot_tree(clf, 
                   feature_names = X_train_num.columns.values,
                   class_names = ['False','True'],
                   filled=True)





# sunny:2, hot:85, normal:65, strong:0 
new_example = [[2, 60, 65, 1]]
X_test = pd.DataFrame(new_example, columns = ['Outlook', 'Temperature', 'Humidity', 'Wind'])
print(X_test)
clf.predict(X_test)
