


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt





df = pd.read_csv("train_housing_data_italy.csv")
print(df.shape)
df.head()


df.isnull().sum()





print(df.duplicated())





print(df.describe())


df.info()





# Precio por metro cuadrado
df['price_per_mq'] = df['price'] / df['mq']

# Antiguedad de la propiedad
current_year = 2024  
df['property_age'] = current_year - df['year_of_construction']





# Categorical columns
cat_col = [col for col in df.columns if df[col].dtype == 'object']
print('Categorical columns :',cat_col)
# Numerical columns
num_col = [col for col in df.columns if df[col].dtype != 'object']
print('\nNumerical columns :',num_col)





# Lista de las variables comparar con 'price'
variables = ['n_rooms', 'floor', 'mq', 'n_bathrooms', 'year_of_construction',
             'has_garage', 'has_terrace', 'has_garden', 'has_balcony', 'has_fireplace', 'has_alarm', 
             'has_air_conditioning', 'has_pool', 'has_parking', 'has_elevator', 'is_furnished']

# Crear subplots
fig, axes = plt.subplots(nrows=len(variables), ncols=1, figsize=(8, 5 * len(variables)))

# Crear un gráfico de dispersión para cada variable contra 'price'
for i, var in enumerate(variables):
    sns.scatterplot(data=df, x=var, y='price', ax=axes[i])
    axes[i].set_title(f'Relación entre {var} y price')

# Ajustar el diseño
plt.tight_layout()
plt.show()





# 1. Eliminar las filas donde 'price' es nulo (variable objetivo)
df_cleaned = df.dropna(subset=['price'])
df_cleaned = df_cleaned.dropna(subset=['year_of_construction'])
df_cleaned = df_cleaned.dropna(subset=['floor'])
df_cleaned.info()


for col in cat_col:
    print(f'Columna {col}: {df[col].nunique()} subniveles')





# 4. Eliminar columnas irrelevantes como 'timestamp', 'title', 'status', 'availability'
df_cleaned = df_cleaned.drop(columns=['timestamp', 'title', 'status','availability'])


df_cleaned = df_cleaned.dropna()
df_cleaned.isnull().sum()


plt.boxplot(df_cleaned['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_cleaned['price'].mean()
std  = df_cleaned['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_1 = df_cleaned[df_cleaned['price'] <= upper_bound]


plt.boxplot(df_1['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_1['price'].mean()
std  = df_1['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_2 = df_1[df_1['price'] <= upper_bound]


plt.boxplot(df_2['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_2['price'].mean()
std  = df_2['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_3 = df_2[df_2['price'] <= upper_bound]


plt.boxplot(df_3['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# Eliminar filas con 0 en la cantidad de cuartos
df_5 = df_3[df_3['n_rooms'] > 0]
print(df_5.shape)


# Columnas numericas
print("Before: ",num_col)
num_col.remove('price')
num_col.remove('timestamp')
num_col.remove('latitude')
num_col.remove('longitude')
print("\nAfter: ",num_col)

cat_col.remove('status')
cat_col.remove('title')
cat_col.remove('availability')


df_5.head()





from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.preprocessing import LabelEncoder

y = df_5['price'].values

label_encoder = LabelEncoder()
X_encoded = df_5[cat_col].apply(lambda col: LabelEncoder().fit_transform(col))

# ** para expandir las columnas numéricas como argumentos de palabra clave dentro de assign(), de modo que se añadan correctamente.
X_complete = X_encoded.assign(**df_5[num_col])

print(len(y))
print(len(X_complete))

print(y)
print(X_complete)


X_complete = X_complete - X_complete.mean()








from sklearn.decomposition import PCA


X_PCA = PCA(n_components=2).fit_transform(X_complete)


scatter = plt.scatter(X_PCA[:,0], X_PCA[:,1], c=y)
plt.title('PCA plot in 2D')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())

#fig = plt.figure()
#ax = plt.axes(projection ="3d")

#ax.scatter3D(X_PCA[:,0], X_PCA[:,1], X_PCA[:,2], c=y)
#plt.title("PCA plot in 3D")

plt.show()





X_LDA = LDA(n_components=2).fit_transform(X_complete,y)

scatter = plt.scatter(X_LDA[:,0], X_LDA[:,1], c=y)
plt.title('LDA plot in 2D')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())

#fig = plt.figure()
#ax = plt.axes(projection ="3d")

#ax.scatter3D(X_LDA[:,0], X_LDA[:,1], X_LDA[:,2], c=y)
#plt.title("LDA plot in 3D")
plt.show()





from sklearn.manifold import TSNE

X_TSNE = TSNE(n_components=2,perplexity=10, early_exaggeration=12, learning_rate='auto', init='pca',n_jobs=8).fit_transform(X_complete)

scatter = plt.scatter(X_TSNE[:,0], X_TSNE[:,1], c=y)
plt.title('t-SNE plot in 2D')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())

#fig = plt.figure()
#ax = plt.axes(projection ="3d")

#ax.scatter3D(X_TSNE[:,0], X_TSNE[:,1], X_TSNE[:,2], c=y)
#plt.title("t-SNE plot in 3D")

plt.show()






from umap import UMAP

X_UMAP = UMAP(n_components=2).fit_transform(X_complete)

scatter = plt.scatter(X_UMAP[:,0], X_UMAP[:,1], c=y)
plt.title('UMAP plot in 2D')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(*scatter.legend_elements())

#fig = plt.figure()
#ax = plt.axes(projection ="3d")

#ax.scatter3D(X_UMAP[:,0], X_UMAP[:,1], X_UMAP[:,2], c=y)
#plt.title("UMAP plot in 3D")

plt.show()





from sklearn.feature_selection import mutual_info_classif

### Applying feature selection method
X_new = mutual_info_classif(X_complete,y)

# plot feature selection
feat_imp = pd.Series(X_new, index=df_5.columns[:len(X_new)])
feat_imp.plot(kind='barh')
plt.show()


from sklearn.feature_selection import VarianceThreshold

#X = df_4.iloc[:, 0:-1]
### Applying feature selection method
sel = VarianceThreshold(threshold=0.16)
%time sel.fit_transform(X_complete)

# show feature selection
selected_features = sel.get_support()
print('The selected features are:', list(X_complete.columns[selected_features]))












from sklearn.model_selection import train_test_split

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = train_test_split(X_complete, y, test_size=0.2, random_state=42)


from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

ffs = SequentialFeatureSelector(lr, n_features_to_select='auto', direction='forward')
ffs.fit(X_complete, y)

# show feature selection
selected_features = ffs.get_support()
print('The selected features are:', list(X_complete.columns[selected_features]))


reg = LinearRegression().fit(X_train, y_train)
print(reg.score(X_train, y_train))


print(reg.coef_)
print(reg.intercept_)
y_pred = reg.predict(X_train)


# Plot the regression line with actual data pointa
plt.plot(X_train, y_train, '+', label='Actual values')
plt.plot(X_train, y_pred, label='Predicted values')
plt.xlabel('Test input')
plt.ylabel('Test Output or Predicted output')
plt.legend()
plt.show()


from sklearn.metrics import r2_score
print(f'R^2: {r2_score(y_train, y_pred)}')



# Function to evaluate models and plot predictions
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    print(f'Model: {model_name}')
    print(f'Train R^2: {r2_score(y_train, y_pred_train):.4f}')
    print(f'Test R^2: {r2_score(y_test, y_pred_test):.4f}')
    print(f'Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}')
    print('-'*40)
    
    # Plot the predictions vs real values
    plt.figure(figsize=(4, 4))
    plt.scatter(y_test, y_pred_test, label='Predictions', color='blue')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title(f'Actual vs Predicted - {model_name}')
    plt.legend()
    plt.show()


from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# 1. Linear Regression
linear_reg = LinearRegression()
evaluate_model(linear_reg, X_train, X_test, y_train, y_test, 'Linear Regression')

# 2. Ridge Regression
ridge_reg = Ridge(alpha=1.0)
evaluate_model(ridge_reg, X_train, X_test, y_train, y_test, 'Ridge Regression')

# 3. Lasso Regression
lasso_reg = Lasso(alpha=0.1)
evaluate_model(lasso_reg, X_train, X_test, y_train, y_test, 'Lasso Regression')

# 4. Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
evaluate_model(rf_reg, X_train, X_test, y_train, y_test, 'Random Forest Regression')





# Cargar el dataset de prueba
test_data = pd.read_csv('./test_housing_data_italy.csv')

# Verificar las primeras filas del conjunto de prueba
print(test_data.head())



test_data = test_data.dropna()


# Aplicar el preprocesamiento correspondiente (por ejemplo, imputación y escalado)
test_data['price_per_mq'] = test_data['price'] / test_data['mq']
test_data['property_age'] = 2024 - test_data['year_of_construction']

y_true = test_data['price']


test_data = test_data.dropna(subset=['price'])
test_data = test_data.dropna(subset=['year_of_construction'])
test_data = test_data.dropna(subset=['floor'])


test_data = test_data.drop(columns=['timestamp', 'title', 'status','availability','longitude', 'latitude', 'price'])



#Se toman las mismas columnas que las del entrenamiento
test_data = test_data[X_train.columns]
print(test_data)


# Codificar categoricos
cat_col = [col for col in test_data.columns if test_data[col].dtype == 'object']

test_data[cat_col] = test_data[cat_col].apply(lambda col: LabelEncoder().fit_transform(col))


from sklearn.metrics import (confusion_matrix, accuracy_score)
 # defining the dependent and independent variables
X_test = test_data[[col for col in test_data.columns if test_data[col].dtype != 'object']]
y_test = y_true


# Predicciones usando los modelos entrenados

# Regresión lineal
y_test_pred_linear = linear_reg.predict(X_test)

# Regresión Ridge
y_test_pred_ridge = ridge_reg.predict(X_test)

# Bosque aleatorio
y_test_pred_rf = rf_reg.predict(X_test)



from sklearn.metrics import r2_score

# Calcular el R² para cada modelo
r2_linear_test = r2_score(y_true, y_test_pred_linear)
r2_ridge_test = r2_score(y_true, y_test_pred_ridge)
r2_rf_test = r2_score(y_true, y_test_pred_rf)

# Imprimir los resultados
print(f"R² de Regresión Lineal en test: {r2_linear_test}")

print(f"R² de Regresión Ridge en test: {r2_ridge_test}")
print(f"R² de Bosque Aleatorio en test: {r2_rf_test}")



