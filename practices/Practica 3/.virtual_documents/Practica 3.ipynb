


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d





df = pd.read_csv("train_housing_data_italy.csv")
print(df.shape)
df.head()


df.isnull().sum()





print(df.duplicated())


print(df.describe())


df.info()


# Categorical columns
cat_col = [col for col in df.columns if df[col].dtype == 'object']
print('Categorical columns :',cat_col)
# Numerical columns
num_col = [col for col in df.columns if df[col].dtype != 'object']
print('Numerical columns :',num_col)





sns.pairplot(df[['price', 'n_rooms', 'floor', 'mq', 'n_bathrooms', 'year_of_construction']])
plt.show()





# 1. Eliminar las filas donde 'price' es nulo (variable objetivo)
df_cleaned = df.dropna(subset=['price'])
df_cleaned = df_cleaned.dropna(subset=['year_of_construction'])
df_cleaned = df_cleaned.dropna(subset=['floor'])

df_cleaned.info()


for col in cat_col:
    print(f'Columna {col}: {df[col].nunique()} subniveles')





# 4. Eliminar columnas irrelevantes como 'timestamp', 'title' y status
df_cleaned = df_cleaned.drop(columns=['timestamp', 'title', 'status'])


# Eliminar duplicados
print(f'Tamaño antes de eliminar las filas repetidas: {df_cleaned.shape}')
df_cleaned = df_cleaned.drop_duplicates()
print(f'Tamaño después de eliminar las filas repetidas: {df_cleaned.shape}')
df_cleaned.head()


plt.boxplot(df_cleaned['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_cleaned['price'].mean()
std  = df_cleaned['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_1 = df_cleaned[df_cleaned['price'] <= upper_bound]


plt.boxplot(df_1['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_1['price'].mean()
std  = df_1['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_2 = df_1[df_1['price'] <= upper_bound]


plt.boxplot(df_2['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_2['price'].mean()
std  = df_2['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_3 = df_2[df_2['price'] <= upper_bound]


plt.boxplot(df_3['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# calculate summary statistics
mean = df_3['price'].mean()
std  = df_3['price'].std()

# Calculate the lower and upper bounds
upper_bound = mean + std*2
 
print('Upper Bound :',upper_bound)
 
# Drop the outliers
df_4 = df_3[df_3['price'] <= upper_bound]


plt.boxplot(df_4['price'], vert=False)
plt.ylabel('Variable')
plt.xlabel('price')
plt.title('Before')
plt.show()


# Eliminar filas con 0 en la cantidad de cuartos
df_4 = df_4[df_4['n_rooms'] > 0]
print(df_4.shape)





from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

num_col.remove('price')
num_col.remove('timestamp')

df_4.dropna(inplace=True)

X = df_4[[num for num in num_col]].values
y = df_4['price'].values

print(len(y))
print(len(X))

print(y)
print(X)





from sklearn.decomposition import PCA

X_PCA = PCA(n_components=3).fit_transform(X)

fig = plt.figure()
ax = plt.axes(projection ="3d")

ax.scatter3D(X_PCA[:,0], X_PCA[:,1], X_PCA[:,2], c=y)
plt.title("PCA plot in 3D")

plt.show()





X_LDA = LDA(n_components=3).fit_transform(X,y)

fig = plt.figure()
ax = plt.axes(projection ="3d")

ax.scatter3D(X_LDA[:,0], X_LDA[:,1], X_LDA[:,2], c=y)
plt.title("LDA plot in 3D")
plt.show()





from sklearn.manifold import TSNE

X_TSNE = TSNE(n_components=3,perplexity=10, early_exaggeration=12, learning_rate='auto', init='pca',n_jobs=8).fit_transform(X)

fig = plt.figure()
ax = plt.axes(projection ="3d")

ax.scatter3D(X_TSNE[:,0], X_TSNE[:,1], X_TSNE[:,2], c=y)
plt.title("t-SNE plot in 3D")

plt.show()






from umap import UMAP

X_UMAP = UMAP(n_components=3).fit_transform(X)

fig = plt.figure()
ax = plt.axes(projection ="3d")

ax.scatter3D(X_UMAP[:,0], X_UMAP[:,1], X_UMAP[:,2], c=y)
plt.title("t-SNE plot in 3D")

plt.show()





from sklearn.ensemble import RandomForestClassifier

### Applying feature selection method
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# show feature selection
importances = rf.feature_importances_
forest_importances = pd.Series(importances, index=X.columns)

plt.figure(figsize=(10, 6))
forest_importances.plot(kind='bar')
plt.title('Feature selection using Random Forest')
plt.show()


import warnings
from mlxtend.feature_selection import ExhaustiveFeatureSelector
from sklearn.ensemble import RandomForestClassifier

with warnings.catch_warnings():
    warnings.simplefilter("ignore")

    ### Applying feature selection method
    efs = ExhaustiveFeatureSelector(RandomForestClassifier(), min_features=1, max_features=4) #change max_features
    efs = efs.fit(X, y)

    # show feature selection
    print('The selected features are:', efs.best_feature_names_)





from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

ffs = SequentialFeatureSelector(lr, n_features_to_select='auto', direction='forward')
ffs.fit(X, y2)

# show feature selection
selected_features = ffs.get_support()
print('The selected features are:', list(X.columns[selected_features]))
